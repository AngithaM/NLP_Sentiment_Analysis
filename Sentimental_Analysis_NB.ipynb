{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4015eac-c0f8-48cb-9d29-524656585662",
   "metadata": {},
   "source": [
    "## Task 1: Reading Data\n",
    "\n",
    "The following cell of code reads the texts and the corresponding labels of suggestion/non-suggestion from the CSV file. The first task is to create training and test sets. Use the final $1000$ rows of the data as a test set and the rest of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d97d301-e8f1-4698-925b-920f28d533e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"Warning 4668 is an extra one we have enabled beyond warning level 4.\"', 0), ('\"Make it possible to optionally declare a function OnClick() for live tiles, such that this code is executed when the live tiles is presed without leaving the home screen.\"', 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "df = pd.read_csv('Data.Assignment2.SemEvalTask9SubtaskA.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Set seed for reproducibility and shuffle the rows.\n",
    "np.random.seed(888)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "data = [(text, label) for (idx, text, label) in df.values.tolist()]\n",
    "print(data[:2])\n",
    "# Create training and test sets.\n",
    "train_texts, train_labels = [], []\n",
    "test_texts, test_labels = [], []\n",
    "\n",
    "#################### EDIT BELOW THIS LINE #########################\n",
    "#iterating over all the rows except for the last 1000 rows\n",
    "for d in data[:-1000]:\n",
    "    #creating a list of training data\n",
    "    train_texts.append(d[0])\n",
    "    #creating a list of training labels\n",
    "    train_labels.append(d[1])\n",
    "    #iterating over the last 1000 rows\n",
    "for d in data[-1000:]:\n",
    "    #using the last 1000 rows to create test data\n",
    "    test_texts.append(d[0])\n",
    "    #using the last 1000 rows to create labels\n",
    "    test_labels.append(d[1])\n",
    "\n",
    "#################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1000\n",
    "assert len(train_texts) == len(train_labels) == 5100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156dc77-b67c-4f07-8138-9701b1c7fb62",
   "metadata": {},
   "source": [
    "## Task 2: Data Pre-processing (30 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier.\n",
    "\n",
    "Ans:\n",
    "\n",
    "---\n",
    "\n",
    "You cannot simply work with raw data, no matter what machine learning model you’re using, and there is no right way to do pre-processing.  However, there are some general steps that we can follow. \n",
    "The following are the steps taken and the major things taken into consideration:\n",
    "1.\tTokenize the data\n",
    "In this case we can simply split the data with respect to white spaces using sentence.split(“ “). But this would keep the punctuations. We can instead use this and then make use of the translate function to map all the punctuations to “ ” or empty spaces. The approach taken up here is the regex model which allows us to clean up the strings by keeping the alphanumeric characters. We can also use word_tokenize from the nltk library for this.\n",
    "2.\tNormalizing the case\n",
    "By removing case sensitivity, we’ll read ‘Keep’ and ‘keep’ as a single word, thus reducing vocabulary. However, this will also cause us to lose some distinctions eg: name like ‘Cliff’ and ‘cliff’ will be treated the same. Since we do not have a lot of proper nouns, we can use this in our case. Lower() is used for this purpose.\n",
    "3.\tRemove empty strings and digits and stopwords from the text\n",
    "Irrelevant data in the test can be removed to further reduce the vocabulary and keep only the relevant content.\n",
    "4.\tStemmers and lemmatizers\n",
    "There are both used to trim the words, reduce the vocabulary and focus on the meaning of the word, but each work in different ways. Stemmers will not always return a meaningful word, on the other hand lemmatizers always does. So as we move from bag of words approach to word encodings, we might be better off using lemmatizers instead of stemmers. In general, only minimal clean up is required for more sophisticated word encodings like Word2Vec or Glove.\n",
    "\n",
    "---\n",
    "An implementation of the steps you mentioned above. \n",
    "Libraries used:\n",
    "nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7a2bb2-57cc-40a2-8b91-47961787d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing multiple underscores in a single word to make tokenization more meaningful \n",
    "def remove_(text):\n",
    "    empty_str_removed=[]\n",
    "    for sentence in text:\n",
    "        new_sen=sentence.replace('_', ' ');\n",
    "        empty_str_removed.append(new_sen)\n",
    "    return empty_str_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0d9f72-5187-4a71-b5e1-aed4d8784ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_texts=remove_(train_texts)\n",
    "new_test_texts=remove_(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5407fda2-96b9-4669-92aa-ddbb987aaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize using regular expressions\n",
    "import re\n",
    "def tokenize(text):\n",
    "    tokenized_text=[]\n",
    "    for sentence in text:\n",
    "        sentence=re.split(r'\\W+', sentence)\n",
    "        tokenized_text.append(sentence)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d0bf18-8657-4ec2-85d0-930d1a78e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_train=tokenize(new_train_texts)\n",
    "tokenized_text_test=tokenize(new_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5faecd8a-904d-44cd-b92c-c6f828a895f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "    # remove empty strings\n",
    "    empty_str_removed=[]\n",
    "    for str_list in text:\n",
    "        str_list = list(filter(None, str_list))\n",
    "        empty_str_removed.append(str_list)\n",
    "    #remove digits\n",
    "    no_integers=[]\n",
    "    for str_list in empty_str_removed:\n",
    "        str_list = [strgs for strgs in str_list if not strgs.isdigit()]\n",
    "        no_integers.append(str_list)\n",
    "    # turn to lower case\n",
    "    to_lower=[]\n",
    "    for str_list in no_integers:\n",
    "        str_list = [strgs.lower() for strgs in str_list]\n",
    "        to_lower.append(str_list)\n",
    "    return(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0388b35b-d9b6-41e2-84d3-b1e0139ac14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_text=clean(tokenized_text_train)\n",
    "clean_test_text=clean(tokenized_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8fc55b-4e96-4d2d-8c87-619fa1f5e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d74d74d-caca-4906-b59a-556dcbbee550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['me', 'myself', 'our', 'ourselves', \"you're\", \"you'll\", 'your', 'yourself', 'he', 'his', 'she', 'her', 'herself', \"it's\", 'itself', 'them', 'theirs', 'what', 'who', 'this', \"that'll\", 'those', 'is', 'was', 'be', 'being', 'has', 'having', 'does', 'doing', 'an', 'and', 'if', 'because', 'until', 'of', 'by', 'with', 'against', 'into', 'during', 'after', 'below', 'from', 'down', 'out', 'off', 'under', 'further', 'once', 'there', 'where', 'how', 'any', 'each', 'more', 'other', 'such', 'nor', 'only', 'same', 'than', 'very', 't', 'will', 'don', 'should', 'now', 'll', 'o', 've', 'ain', \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "new_stopword=stopwords.words('english')\n",
    "print(len(new_stopword))\n",
    "list_2=['no',\n",
    " 'nor',\n",
    " 'not','about',\n",
    " 'against','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\"hadn't\",'hasn',\"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",'mustn',\"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n",
    " 'wasn', \"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\",'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\",'don', \"don't\", 'should', \"should've\"\n",
    "]\n",
    "for element in new_stopword:\n",
    "    if element in new_stopword:\n",
    "        new_stopword.remove(element)\n",
    "print(new_stopword)\n",
    "print(len(new_stopword))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3c67ac-7c90-43e4-b36d-584ce6b4e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove comman and irrelevant words\n",
    "def remove_stopwords(text):\n",
    "    removed_stopwords=[]\n",
    "    for str_list in text:\n",
    "        filtered_words = [word for word in str_list if word not in new_stopword]\n",
    "        removed_stopwords.append(filtered_words)\n",
    "    return removed_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d13c6005-7255-46f4-99c3-3658432754d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noStop_train_text=remove_stopwords(clean_train_text)\n",
    "noStop_test_text=remove_stopwords(clean_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "196e9d82-a8c7-4ff4-8af7-998f8a1c3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trims the word to a meaningful word\n",
    "def lemmatizer(text):\n",
    "    filtered_words_lemmatize=[]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for str_list in text:\n",
    "        filtered_words = [lemmatizer.lemmatize(word) for word in str_list]\n",
    "        filtered_words_lemmatize.append(filtered_words)\n",
    "    return filtered_words_lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8cdcaee-9cf9-4c73-9b81-388eb0c424d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_train_text=lemmatizer(noStop_train_text)\n",
    "lemmatize_test_text=lemmatizer(noStop_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a5c1e5-ef2f-4181-845f-8dea0661ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuts the word to it's root. But often forms meaningless words\n",
    "def stemmer(text):   \n",
    "    ps = PorterStemmer()\n",
    "    test_filtered_words_stem=[]\n",
    "    for str_list in text:\n",
    "        filtered_words = [ps.stem(word) for word in str_list]\n",
    "        test_filtered_words_stem.append(filtered_words)\n",
    "    return test_filtered_words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edc69437-625a-4adc-b19e-27070faa4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmers not used in either of the models. However, it maybe used for tf-idf. \n",
    "#This was tested and the results were more or less the same.\n",
    "stemmed_train_text=stemmer(lemmatize_train_text)\n",
    "stemmed_test_text=stemmer(lemmatize_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a499cb61-4c7a-4b66-ba3e-7d6fb013f373",
   "metadata": {},
   "outputs": [],
   "source": [
    " def join(text):\n",
    "    test_list_joined=[]\n",
    "    for str_list in text:\n",
    "        str=' '.join(str_list)\n",
    "        test_list_joined.append(str)\n",
    "    return test_list_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f586e8ce-6c4a-463d-b56f-58c67bdab550",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_train_text=join(lemmatize_train_text)\n",
    "join_test_text=join(lemmatize_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8462d7-6a1c-4731-a6e1-98ae56ebad03",
   "metadata": {},
   "source": [
    "## Task 3: Feature Engineering (I) - TF-IDF as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f0bb704-bce4-4ecc-8592-981583d03cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0350ffe2-4cfe-48ac-a152-ebfb8d53df65",
   "metadata": {},
   "outputs": [],
   "source": [
    " def tfidf_converter(list_train,list_test):\n",
    "    #determines the number of features for this test set\n",
    "    cv = CountVectorizer()\n",
    "    #creates a collection of tokens for the model to use\n",
    "    word_count_vector = cv.fit_transform(list_train)\n",
    "    #convert to a useable dataframe\n",
    "    tf = pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names())\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    #fir and transfor the training set according to the features set\n",
    "    X = tfidf_transformer.fit_transform(word_count_vector)\n",
    "    idf = pd.DataFrame({'feature_name':cv.get_feature_names(), 'idf_weights':tfidf_transformer.idf_})\n",
    "    #only transform and not fit the test set to create vectors with the same number of features as \n",
    "    #earlier defined\n",
    "    test_word_count_vector = cv.transform(list_test)\n",
    "    test_tf = pd.DataFrame(test_word_count_vector.toarray(), columns=cv.get_feature_names())\n",
    "    #the test set is transformed\n",
    "    X_test = tfidf_transformer.transform(test_word_count_vector)\n",
    "    return X.toarray(),X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a2abc4f-b1cf-4899-883d-2b879b44f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_test=tfidf_converter(join_train_text,join_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32c6fdfc-a13f-41e1-9183-7c058079789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddb48409-7772-4c00-92ab-5872349aad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNB_predict(X,X_test):\n",
    "    gnb_predicted_test = []# save your predictions on the test set into this list\n",
    "    gnb_predicted_train=[]# to check for underfitting and overfitting\n",
    "    y=train_labels\n",
    "    y_test=test_labels\n",
    "    gb = GaussianNB(var_smoothing=0.075) # naive_bayes model is created\n",
    "    gb.fit(X, y) # data is fited with training data\n",
    "    gnb_predicted_test=gb.predict(X_test) # prediction made on test data\n",
    "    gnb_predicted_train=gb.predict(X)     # training data is also tested\n",
    "    return gb,gnb_predicted_test,gnb_predicted_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d11bafd-f1dc-4e7c-b6ad-1aa732f21608",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb,gnb_predicted_test,gnb_predicted_train=GaussianNB_predict(X,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "568581ff-88ff-48f6-8ef8-eb1cda940917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e55e7f7-9f1a-402d-b493-6d353ca1940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644\n",
      "0.7864705882352941\n"
     ]
    }
   ],
   "source": [
    "y=train_labels\n",
    "y_test=test_labels\n",
    "print(accuracy(y_test, gnb_predicted_test))\n",
    "print(accuracy(y, gnb_predicted_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac1cd0-1b0c-46b4-86f0-dcc3279d6c1c",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681e09a-06b0-4708-a9b1-9c5992c92914",
   "metadata": {},
   "source": [
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b34f79bd-fdb5-46b3-bd6d-8262b3edd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4015\n",
      "6100\n",
      "2085\n"
     ]
    }
   ],
   "source": [
    "#entire model class distribution\n",
    "labels=test_labels+train_labels\n",
    "print(labels.count(0))\n",
    "print(len(labels))\n",
    "print(labels.count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaa024f2-98c2-4d02-b3b8-e738c1dbde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n",
      "1000\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "#test label class distribution\n",
    "print(test_labels.count(0))\n",
    "print(len(test_labels))\n",
    "print(test_labels.count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdbac7-951a-4a4c-8deb-923f15eda3a2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "When we check the count of each of the distinct labels, we can see that this problem is not balanced with this data. The cases with 0 as the label is almost twice that of 1! So we clearly have a majority class and a minority class. Now if we create a classifier that predits 0 at all times, it will still have a high accuracy rate.(In this case .648 as observed from the test label class distribution). That is why accuracy is not a good way to evaluate the classifier in this case.\n",
    "\n",
    "Better Evaluation Metrics:\n",
    "    Precision-Recall Metrics\n",
    "Precision : Rate of the true positives. Tells us how well the positive classes are predicted.\n",
    "    Precision = TruePositive / (TruePositive + FalseNegative)\n",
    "\n",
    "Recall: Rate of the true negatives. This tells us how well the negative classes are predicted.\n",
    "    Recall = TrueNegative / (FalsePositive + TrueNegative)\n",
    "In this case, precision might be more useful as it will tell you how many were correctly classified.\n",
    "The combination of precision and recal to balance both concerns is the F-score. \n",
    "\n",
    "Confusion Matrix as we know will be really helpful as it gives you the data about true positive and true negatives.\n",
    "\n",
    "All these are better metrics when compared to accuracy and should be used here.\n",
    "ROC Area Under Curve or Precision-Recall Area Under Curve can be used for severely imbalanced problem.\n",
    "\n",
    "(Not under the scope with the current word limit)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc5b9a80-95be-4d8c-ad1f-36a62a7e8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de1d37c3-57da-4dfd-a9e5-2da0a7c0113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y,y_test,pred_train,pred_test):\n",
    "    print('Accuracy of test data: %.3f' % accuracy(y_test, pred_test))\n",
    "    # train data is also evaluated to understand underfitting and overfitting.\n",
    "    #Since the accuracy of training data is high, overfitting is very likely\n",
    "    print('Accuracy of train data: %.3f' % accuracy(y, pred_train)) \n",
    "\n",
    "    print('Precision of test data: %.3f' % precision_score(y_test, pred_test))\n",
    "\n",
    "    print('Recall of test data: %.3f' % recall_score(y_test, pred_test))\n",
    "\n",
    "    print('f1_score of test data: %.3f' % f1_score(y_test, pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a06596e-d22f-458b-95f4-c30824fa51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data: 0.644\n",
      "Accuracy of train data: 0.786\n",
      "Precision of test data: 0.497\n",
      "Recall of test data: 0.832\n",
      "f1_score of test data: 0.622\n"
     ]
    }
   ],
   "source": [
    "evaluate_metrics(y,y_test,gnb_predicted_train,gnb_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be8390e8-c036-4991-80dd-2b2bf5ae72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "098f2879-5043-461e-afc0-828f44dbf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix as we know will be really helpful as it gives you the data about true positive and true negatives.\n",
    "\n",
    "def confusion_matrix_plot(y,X,model):\n",
    "    plot_confusion_matrix(model, X, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c4742d2-343e-4d3b-9a3a-c9c11777ce6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_plot(y,X,gb)\n",
    "\n",
    "#we can see why the recall of the train set is 1 looking at the confusion matrix below\n",
    "#The label predicts 1 more than 0. This is considering the majority class is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6404af62-bcd8-41c2-b631-da3c2d82f404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_plot(y_test,X_test,gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c001b-c697-4d75-a391-b27f129caf7b",
   "metadata": {},
   "source": [
    "# Task 5: Feature Engineering (II) - Other features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96dc30e0-946e-4599-a8e9-671375caa5f6",
   "metadata": {},
   "source": [
    "Describe features other than those defined in Task 3 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce266f5d-83f0-4869-ab03-b6c6b1ad26f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Word embedding is a feature that can help with sentimental analysis as it can help identify the semantic relation ship between words. Here we’re discarding the bag of words approach used in TF_IDF. Normal encodings like one-hot encoding can end up with very high dimensionality vectors, and they will not account for semantics. This is where word embedding comes in. Here each word is given a position in a multi-dimensional space, the number of dimensions is our choice. This position is represented by a dense vector that corresponds to it’s meaning or semantics within the training text.\n",
    "I have used 300 dimensions to try improve the performance. This can be changed to 25d, 50d, 100d, 200d etc in Glove. You can also create your own word embedding using the vocabulary from the dataset. This, however, will not give as good a performance as pre-trained ones like Glove or Word2Vec. This is because the limited vocabulary will not allow the training model to learn properly and understand the relative meanings. (Here the code for word embedding used is mostly taken from the exercise sheet for Lab8.) Each word will correspond to a 300 dimension vector this is then applied to find the vector corresponding to an entire sentence by finding it’s mean. It is this vector that is trained and used for predictions. \n",
    "Pre-processing for word embeddings:\n",
    "Since word embeddings have semantics included in them, very little pre-processing is required like removing stop-words, punctuations and lemmatization. Lemmatization is used only to reduce the complexity since the variations in spellings, case and grammar will be captures and similar words will occupy close positions . Stemming should not be done in this case as it could generate meaningless word that will not be included in the pre-trained word list. If you’re not using pre-trained vectors, but rather creating your own, stemming might still be okay.\n",
    "Since the evaluation metrics still yields low values, we can try to combine features by stacking them together. So, in this case we combine tf-idf and word embedding together and  the model is trained again. The result is slightly better due to more number of features. \n",
    "We can also try altering the dimensions from 300d to 200 d and 100 d or increase it. Another model is the 2-gram or n-gram model which means taking 2-consecutive/n-consecutive words together and using them as building blocks for our model.\n",
    "We can also use a variation of Naive Bayes like the Multinomial Naive Bayes which uses a probabilistic learning method \n",
    "But due to the imbalanced data, the performance is not as good as can be expected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62bc4135-4cba-4f6e-bc99-60db9243d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd696554-73f8-4860-89f3-6f55edf69c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\angit\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\angit\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\angit\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\angit\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\angit\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ea48b8a-86ce-4999-9286-721f3ad38f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading glove embeddings. please note this may take a few minutes\n",
      "finished downloading glove\n"
     ]
    }
   ],
   "source": [
    "# code taken from Exercise sheet 8 and modified for this task\n",
    "\n",
    "import gensim.downloader as api\n",
    "print(\"downloading glove embeddings. please note this may take a few minutes\")\n",
    "glove = api.load('glove-wiki-gigaword-300')\n",
    "print(\"finished downloading glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb456c5c-59f6-4897-a317-38fa3a3bd75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(lemmatize_train_text))\n",
    "print(len(lemmatize_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b659118e-4d9b-48f9-8e7d-7bb114dd802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from Exercise sheet 8 and modified for this task\n",
    "def vectorize(text):\n",
    "    all_text_vecs = []\n",
    "    oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "\n",
    "    # 1. Loop over text toks. \n",
    "    for toks in text:\n",
    "      train_vecs = []\n",
    "\n",
    "      # 2. Loop over each word in the tokenzed input\n",
    "      for tok in toks:\n",
    "        # If token exists glove, add its embedding to text_vecs\n",
    "        if tok in glove:\n",
    "          train_vecs.append(glove[tok])\n",
    "\n",
    "      # 3. Once you extracted all the word embedding for the input\n",
    "      # append text_vecs to all_text_vecs in the else condition \n",
    "      if train_vecs == []:\n",
    "        # special case where list is empty and no word embeddings were found\n",
    "        all_text_vecs.append(oov)\n",
    "      else:\n",
    "        all_text_vecs.append(train_vecs)\n",
    "    \n",
    "    print(\"finished\")\n",
    "    print(f\"Check length of all_text_vecs == length of text toks: {len(all_text_vecs)==len(text)}\")\n",
    "    return all_text_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4faffafe-0993-4ed4-b82c-5d98f61e47bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "Check length of all_text_vecs == length of text toks: True\n",
      "finished\n",
      "Check length of all_text_vecs == length of text toks: True\n"
     ]
    }
   ],
   "source": [
    "vec_test=vectorize(lemmatize_test_text)\n",
    "vec_train=vectorize(lemmatize_train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e57f1b4-9385-4f1b-8da9-0ad90baeb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vec(all_text_vecs):\n",
    "    all_pooled_vecs = []\n",
    "\n",
    "    # 1. Loop over each list of word embeddings per input\n",
    "    for text_vecs in all_text_vecs:\n",
    "      # 2. Vstack and take the mean of the tex vecs\n",
    "      mean_pool = np.mean(np.vstack(text_vecs), axis=0)\n",
    "\n",
    "      # 3. Append the mean pooled vector to all_pooled_vecs\n",
    "      all_pooled_vecs.append(mean_pool)\n",
    "\n",
    "    # 4 Update dataset with these pooled vectors\n",
    "    print(len(all_pooled_vecs))\n",
    "    print(\"finished\")\n",
    "    print(f\"sanity check {len(all_pooled_vecs) == len(all_text_vecs)}\")\n",
    "    print(f\"sanity check {np.vstack(all_pooled_vecs).shape[1] == 300}\")\n",
    "    return all_pooled_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5515c5be-2cd3-4663-b544-ed84556b2e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "finished\n",
      "sanity check True\n",
      "sanity check True\n",
      "5100\n",
      "finished\n",
      "sanity check True\n",
      "sanity check True\n"
     ]
    }
   ],
   "source": [
    "X_vec_mean_test=mean_vec(vec_test)\n",
    "X_vec_mean_train=mean_vec(vec_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0bf38af9-6659-4c22-9c7f-8fb7ba3fa183",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1,gnb_pred_vec_test,gnb_pred_vec_train=GaussianNB_predict(X_vec_mean_train,X_vec_mean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f8aab51-18da-43c0-9ba8-e3715f23af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data: 0.456\n",
      "Accuracy of train data: 0.440\n",
      "Precision of test data: 0.390\n",
      "Recall of test data: 0.972\n",
      "f1_score of test data: 0.557\n"
     ]
    }
   ],
   "source": [
    "evaluate_metrics(y,y_test,gnb_pred_vec_train,gnb_pred_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b23a24f6-18a7-4774-803f-05866dad4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 6683)\n",
      "(5100, 300)\n"
     ]
    }
   ],
   "source": [
    "X_2_test=np.array(X_vec_mean_test)\n",
    "X_2=np.array(X_vec_mean_train)\n",
    "\n",
    "X_1=np.array(X)\n",
    "X_1_test=np.array(X_test)\n",
    "print(X_1.shape)\n",
    "print(X_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa96f609-bbc0-42e8-9875-e58e18bd7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 6983)\n",
      "(1000, 6983)\n"
     ]
    }
   ],
   "source": [
    "#horizontally stacking to combine all the features created.\n",
    "X_train_comb = np.hstack((X_1,X_2))\n",
    "X_test_comb = np.hstack((X_1_test,X_2_test))\n",
    "#X_test_comb = np.hstack((X_test, X_test_new))\n",
    "print(X_train_comb.shape)\n",
    "print(X_test_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38058d15-dc81-4dd6-ad37-1907b2553440",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2,gnb_pred_vec_test_comb,gnb_pred_vec_train_comb=GaussianNB_predict(X_train_comb,X_test_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e801058c-86d5-4f78-8e66-e4fe45229118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data: 0.456\n",
      "Accuracy of train data: 0.447\n",
      "Precision of test data: 0.390\n",
      "Recall of test data: 0.972\n",
      "f1_score of test data: 0.557\n"
     ]
    }
   ],
   "source": [
    "#evaluation of the new model\n",
    "#precision and accuracy are low, but f1_score indicates that overfitting does not occur anymore.\n",
    "evaluate_metrics(y,y_test,gnb_pred_vec_train_comb,gnb_pred_vec_test_comb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e2ecdc2-7e13-4d12-a9d0-84dc1ba57cb0",
   "metadata": {},
   "source": [
    "The final input to the model has 6924 features that is 300 more than the initial features passed to the model. Perhaps the bias is more towards the tf-idf model in this case. The metric has improved and will be giving better for a different set of data than the original tf-idf model as that was severely overfitting and could not successfully generalize the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f29ced-e947-41b3-916c-c15494506231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
